
## Activation Functions

- Originally tanh [sigmoid or logistic sigmoid?]
- Now usually Rectified Linear Activation Unit (ReLU)  ```__/```
  - RELU(x) = 0 if x < 0, x if x >= 0






